<?xml version="1.0" encoding="UTF-8" ?>
<ChoregrapheProject xmlns="http://www.aldebaran-robotics.com/schema/choregraphe/project.xsd" xar_version="3">
    <Box name="root" id="-1" localization="8" tooltip="Root box of Choregraphe&apos;s behavior. Highest level possible." x="0" y="0">
        <bitmap>media/images/box/root.png</bitmap>
        <script language="4">
            <content>
                <![CDATA[]]>
</content>
        </script>
        <Input name="onLoad" type="1" type_size="1" nature="0" inner="1" tooltip="Signal sent when diagram is loaded." id="1" />
        <Input name="onStart" type="1" type_size="1" nature="2" inner="0" tooltip="Box behavior starts when a signal is received on this input." id="2" />
        <Input name="onStop" type="1" type_size="1" nature="3" inner="0" tooltip="Box behavior stops when a signal is received on this input." id="3" />
        <Output name="onStopped" type="1" type_size="1" nature="1" inner="0" tooltip="Signal sent when box behavior is finished." id="4" />
        <Timeline enable="0">
            <BehaviorLayer name="behavior_layer1">
                <BehaviorKeyframe name="keyframe1" index="1">
                    <Diagram>
                        <Box name="HumanGreeter2" id="1" localization="8" tooltip="" x="251" y="160">
                            <bitmap>media/images/box/box-python-script.png</bitmap>
                            <script language="4">
                                <content>
                                    <![CDATA[# -*- encoding: UTF-8 -*-
""" Say 'hello, you' each time a human face is detected

"""

import sys
import time
import re
import subprocess
import json
import httplib, urllib, base64

from naoqi import ALProxy
from naoqi import ALBroker
from naoqi import ALModule


# Global variables. Module names must be global otherwise the callback functions will report for example:
# 'python object not found FaceDetector'.
SpeechRecognizer = None
FaceDetector = None
ReactToTouch = None


# runShellCommandWait(cmd) will block until 'cmd' is finished.
# This because the communicate() method is used to communicate to interact with the process through the redirected pipes.
def runShellCommandWait(cmd):
    return subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, shell=True).communicate()[0]


def testConnected():
    try :
        stri = "http://www.google.com"
        data = urllib.urlopen(stri)
        return True
    except Exception, e:
        return False


# The below class is an alternative to the Nao speech recognition.
# The Nao speech recognition only works with predefined words while with the Google Speech to Text
# all speech can be recognized. The below class is not a module in the NAOqi sense because we do not
# register it with ALModule.__init__(). This is not needed as we do not use subscribeToEvent().
# Instead we just wait for Google to respond.
class speechRecognitionGoogleModule():
    def __init__(self):
        try:
            self.recorder = ALProxy("ALAudioRecorder")
            self.player = ALProxy('ALAudioPlayer')
            self.leds = ALProxy("ALLeds")
        except Exception, e:
            print str(e)
    def speechToText(self):
        text = ""
        try:
            # Stop recording if recorder is still recording, added for robustness.
            try:
                self.recorder.stopMicrophonesRecording()
            except Exception, e:
                pass
            self.player.playFile("/usr/share/naoqi/wav/begin_reco.wav")
            # Start recording 16000 Hz, ogg format, front microphone.
            self.recorder.startMicrophonesRecording("/home/nao/speech.ogg", "ogg", 16000, (0,0,1,0))
            for _ in range(8):
            # Twinkle eye leds to indicate Nao is listening.
                self.leds.fadeRGB("FaceLeds", 255*256*0 + 256*0 + 0, 0.2)
                self.leds.fadeRGB("FaceLeds", 255*256*0 + 256*0 + 255, 0.2)
            self.recorder.stopMicrophonesRecording()
            self.player.playFile("/usr/share/naoqi/wav/end_reco.wav")
            runShellCommandWait('ffmpeg -y -i /home/nao/speech.ogg /home/nao/speech.flac')
            stdOutAndErr = runShellCommandWait('curl -s -X POST --header "content-type: audio/x-flac; rate=16000;" --data-binary @"/home/nao/speech.flac" "http://www.google.com/speech-api/v2/recognize?client=chromium&lang=en_US&key=AIzaSyC3qc74SxJI7fIv747QPlSQPS0rl4AnSAM"')
        except Exception, e:
            print str(e)
            return text
        # Google always replies with an empty JSON response '{"result":[]}' on the first line.
        # If there is speech, The second line contains the actual JSON result.
        # If there is no speech, there is no second line so we have to check this.
        if len(stdOutAndErr.splitlines()) != 2:
            # Return empty text to indicate the speech is invalid.
            return text
        stdOutAndErr = stdOutAndErr.splitlines()[1]
        # Now stdOutAndErr contains the JSON response from the STT engine.
        decoded = json.loads(stdOutAndErr)
        try:
            confidence = decoded["result"][0]["alternative"][0]["confidence"]  # not a string but a float
        except Exception, e:
            print str(e)
            pass
        try:
            # Use encode() to convert the Unicode strings contained in JSON to ASCII.
            text = decoded["result"][0]["alternative"][0]["transcript"].encode('ascii', 'ignore')
        except Exception, e:
            print str(e)
            pass

        return text


class ReactToTouchModule(ALModule):
    """ A simple module able to react to sensor events.
    Leave this doc string else this module will not be bound!

    """
    def __init__(self, name):
        ALModule.__init__(self, name)
        # No need for IP and port here because
        # we have our Python broker connected to NAOqi broker
        self.memory = ALProxy("ALMemory")
        # Unsubscribe to event if still subscribed.
        try:
            self.memory.unsubscribeToEvent("TouchChanged", "ReactToTouch")
        except Exception, e:
            pass
        self.memory.subscribeToEvent("TouchChanged", "ReactToTouch", "onTouched")
        self.touched = ""

    def onTouched(self, strVarName, value):
        self.touched = str(value)

    def getTouch(self):
        if self.touched != "":
            tmpTouched = self.touched
            self.touched = ""
            return tmpTouched
        else:
            return ""


class speechRecognitionNaoModule(ALModule):
    """ A simple module able to react to speech events.
    Leave this doc string else this module will not be bound!

    """
    def __init__(self, name):
        ALModule.__init__(self, name)
        # No need for IP and port here because
        # we have our Python broker connected to NAOqi broker
        self.memory = ALProxy("ALMemory")
        # Unsubscribe to event if still subscribed to stop ASR engine.
        try:
            self.memory.unsubscribeToEvent("WordRecognized", "SpeechRecognizer")
        except Exception, e:
            pass
        self.asr = ALProxy("ALSpeechRecognition")
        vocabulary = ["rene", "james"]
        self.asr.setVocabulary(vocabulary, False )
        self.asr.setVisualExpression(True)
        self.word = ""

    def startListening(self):
        # Subscribe to the WordRecognized event:
        try:
            self.memory.subscribeToEvent("WordRecognized", "SpeechRecognizer", "onWordRecognized")
        except Exception, e:
            pass

    def stopListening(self):
        # Unsubscribe to the WordRecognized event:
        try:
            self.memory.unsubscribeToEvent("WordRecognized", "SpeechRecognizer")
        except Exception, e:
            pass

    def speechToText(self):
        self.startListening()
        startTime = int(time.time())
        wordRecognized = ""
        while True:
            if int(time.time()) - startTime > 5:
                # Timeout.
                self.stopListening()
                break
            if self.word != "":
                wordRecognized = self.word
                self.word = ""
                break
            time.sleep(0.5)
        return wordRecognized

    def onWordRecognized(self, key, value, message):
        if(len(value) > 1 and value[1] >= 0.3):
            # Stop speech recognition.
            self.stopListening()
            self.word = value[0]

class FaceDetectionModule(ALModule):
    """ A simple module able to react to facedetection events.
    Leave this doc string else this module will not be bound!

    """
    def __init__(self, name):
        ALModule.__init__(self, name)
        # No need for IP and port here because
        # we have our Python broker connected to NAOqi broker
        self.memory = ALProxy("ALMemory")
        # Unsubscribe to event if still subscribed.
        try:
            self.memory.unsubscribeToEvent("FaceDetected", "FaceDetector")
        except Exception, e:
            pass
        self.tracking = ALProxy("ALTracker")
        self.timeFaceDetectionStartedInSeconds = 0
        self.timeFirstRecognitionInSeconds = 0
        self.previousTimeInSeconds = 0
        self.timeInSeconds = 0
        self.face = ""
        self.tracker = ALProxy("ALTracker")
        self.motion = ALProxy("ALMotion")
        self.leds = ALProxy("ALLeds")

    def learnFace(self, face):
        # Create a proxy to ALFaceDetection
        try:
            faceProxy = ALProxy("ALFaceDetection")
            # First forget face to be enable learning it again.
            faceProxy.forgetPerson(face)
            faceProxy.learnFace(face)
        except Exception, e:
            print str(e)


    def unlearnAllFaces(self):
        # Create a proxy to ALFaceDetection
        try:
            faceProxy = ALProxy("ALFaceDetection")
            faceProxy.clearDatabase()
        except Exception, e:
            print str(e)

    def lookForward(self):
        self.tracker.lookAt([1.0, 0.0, 0.0], 0, 0.20, False)

    def startFaceDetection(self):
        # Subscribe to the FaceDetected event:
        try:
            self.timeFaceDetectionStartedInSeconds = int(time.time())
            self.memory.subscribeToEvent("FaceDetected", "FaceDetector", "onFaceDetected")
            # Set eye leds.
            self.leds.fadeRGB("FaceLeds", 255*256*0 + 256*0 + 255, 0.2)
        except Exception, e:
            print str(e)

    def stopFaceDetection(self):
        # Unsubscribe to the FaceDetected event:
        try:
            self.memory.unsubscribeToEvent("FaceDetected", "FaceDetector")
            self.leds.fadeRGB("FaceLeds", 255*256*100 + 256*100 + 100, 0.2)
        except Exception, e:
            print str(e)

    def startFaceTracking(self):
        try:
            # Start face tracking.
            self.tracker.registerTarget("Face", 0.2)
            self.tracker.setMode("Head")  # Only move head.
            self.tracker.track("Face")
            # Set stiffness of head to 1.0 as this is required for face tracking.
            self.motion.setStiffnesses("Head", 1.0)
        except Exception, e:
            print str(e)
            pass

    def stopFaceTracking(self):
        try:
            # Stop face tracking.
            self.tracker.stopTracker()
            self.tracker.unregisterTarget("Face")
        except Exception, e:
            print str(e)
            pass

    def startSoundTracking(self):
        try:
            # Start face tracking.
            self.soundLocation = ALProxy("ALSoundLocalization")
            self.soundLocation.setParameter("Sensitivity", 0.7)
            self.tracker.setEffector("None")
            self.tracker.registerTarget("Sound", [0.5, 0.1])
            self.tracker.setRelativePosition([-0.3, 0.0, 0.0, 0.1, 0.1, 0.3])
            self.tracker.setMode("Head")  # Only move head.
            self.tracker.track("Sound")
            # Set stiffness of head to 1.0 as this is required for face tracking.
            self.motion.setStiffnesses("Head", 1.0)
        except Exception, e:
            print str(e)
            pass

    def stopSoundTracking(self):
        try:
            # Stop face tracking.
            self.tracker.setEffector("None")
            self.tracker.stopTracker()
            self.tracker.unregisterTarget("Sound")
        except Exception, e:
            print str(e)
            pass

    def getFace(self):
        if self.face != "":
            tmpFace = self.face
            self.face = ""
            return tmpFace
        else:
            return ""

    def onFaceDetected(self, *_args):
        """ This will be called each time a face is
        detected.
        Leave this doc string else this method will not be bound!
        """
        val = self.memory.getData("FaceDetected")
        if(val and isinstance(val, list) and len(val) >= 2):
            # Stop face detection.
            self.stopFaceDetection()
            # indicata a face is detected.
            self.face = "detected"


class FaceRecognition():
    def __init__(self):
        self.key = '43acf60f71204ca78c4c09a1cb2c6916'

    def getFaceIdFromNewFace(self, pic):
        headers = {
            # Request headers
            'Content-Type': 'application/octet-stream',
            'Ocp-Apim-Subscription-Key': self.key,
        }

        params = urllib.urlencode({
            'returnFaceLandmarks': 'true',
            'returnFaceAttributes': 'age',
        })

        faceId = None
        try:
            conn = httplib.HTTPSConnection('api.projectoxford.ai')
            conn.request("POST", "/face/v1.0/detect?%s" % params, open(pic,"rb").read(), headers)
            response = conn.getresponse()
            data = response.read()
            conn.close()
            decoded = json.loads(data)
            # Use encode() to convert the Unicode strings contained in JSON to ASCII.
            try:
                faceId = decoded[0]["faceId"].encode('ascii', 'ignore')
            except:
                pass
            return faceId
        except Exception,e:
            print "getFaceIdFromNewFace exception: " + str(e)
            return faceId

    def createFaceList(self, faceListId, facelistName):
        headers = {
            # Request headers
            'Content-Type': 'application/json',
            'Ocp-Apim-Subscription-Key': self.key,
        }

        params = urllib.urlencode({
        })

        body = '{\
            "name":"' + facelistName + '",\
            "userData":"User-provided data attached to the face list"\
        }'

        try:
            conn = httplib.HTTPSConnection('api.projectoxford.ai')
            conn.request("PUT", "/face/v1.0/facelists/" + faceListId + "?%s" % params, body, headers)
            response = conn.getresponse()
            data = response.read()
            conn.close()
            return True
        except Exception,e:
            print "createFaceList exception: " + str(e)
            return False

    def deleteFaceList(self, faceListId):
        headers = {
            # Request headers
            'Ocp-Apim-Subscription-Key': self.key,
        }

        params = urllib.urlencode({
        })

        try:
            conn = httplib.HTTPSConnection('api.projectoxford.ai')
            conn.request("DELETE", "/face/v1.0/facelists/" + faceListId + "?%s" % params, "{body}", headers)
            response = conn.getresponse()
            data = response.read()
            conn.close()
            return True
        except Exception,e:
            print "deleteFaceList exception: " + str(e)
            return False

    def addFaceToList(self, faceListId, pic, userData):
        headers = {
            # Request headers
            'Content-Type': 'application/octet-stream',
            'Ocp-Apim-Subscription-Key': self.key,
        }

        params = urllib.urlencode({
            # Request parameters
            'userData': userData,
        })

        try:
            conn = httplib.HTTPSConnection('api.projectoxford.ai')
            conn.request("POST", "/face/v1.0/facelists/" + faceListId + "/persistedFaces?%s" % params, open(pic,"rb").read(), headers)
            response = conn.getresponse()
            data = response.read()
            print "added face: " + data
            conn.close()
            return True
        except Exception,e:
            print "addFaceToList exception: " + str(e)
            return False

    def deleteFaceFromList(self, faceListId, name):
        faceId = self.nameToFaceId(faceListId, name)
        headers = {
            # Request headers
            'Ocp-Apim-Subscription-Key': self.key,
        }

        params = urllib.urlencode({
        })

        try:
            conn = httplib.HTTPSConnection('api.projectoxford.ai')
            conn.request("DELETE", "/face/v1.0/facelists/" + faceListId + "/persistedFaces/" + faceId + "?%s" % params, "", headers)
            response = conn.getresponse()
            data = response.read()
            conn.close()
            return True
        except Exception,e:
            print "deleteFaceFromList exception: " + str(e)
            return False

    def getRecognizedFaceId(self, faceListId, newFaceId):
        headers = {
            # Request headers
            'Content-Type': 'application/json',
            'Ocp-Apim-Subscription-Key': self.key,
        }

        params = urllib.urlencode({
        })

        body = '{"faceId":"' + newFaceId + '",\
            "faceListId":"' + faceListId + '",\
            "maxNumOfCandidatesReturned":1\
        }'

        faceId = ""
        confidence = 0
        try:
            conn = httplib.HTTPSConnection('api.projectoxford.ai')
            conn.request("POST", "/face/v1.0/findsimilars?%s" % params, body, headers)
            response = conn.getresponse()
            data = response.read()
            conn.close()
            decoded = json.loads(data)
            try:
                # Use encode() to convert the Unicode strings contained in JSON to ASCII.
                faceId = decoded[0]["persistedFaceId"].encode('ascii', 'ignore')
                confidence = decoded[0]["confidence"]
            except:
                pass
            return faceId, confidence
        except Exception,e:
            print "getRecognizedFaceId exception: " + str(e)
            return faceId, confidence

    def getFaceList(self, faceListId):
        headers = {
            # Request headers
            'Ocp-Apim-Subscription-Key': self.key,
        }

        params = urllib.urlencode({
        })

        decoded = None
        try:
            userData = ""
            conn = httplib.HTTPSConnection('api.projectoxford.ai')
            conn.request("GET", "/face/v1.0/facelists/" + faceListId + "?%s" % params, "", headers)
            response = conn.getresponse()
            data = response.read()
            conn.close()
            decoded = json.loads(data)
            return decoded
        except Exception,e:
            print "getFaceList exception: " + str(e)
            return decoded

    def getFaceNames(self, faceListId):
        faceNames = []
        decoded = self.getFaceList(faceListId)
        if decoded:
            try:
                for face in decoded["persistedFaces"]:
                    # Use encode() to convert the Unicode strings contained in JSON to ASCII.
                    faceNames.append(face["userData"].encode('ascii', 'ignore'))
            except Exception,e:
                pass
        return faceNames

    def faceIdToName(self, faceListId, faceId):
        name = ""
        decoded = self.getFaceList(faceListId)
        if decoded:
            try:
                for face in decoded["persistedFaces"]:
                    if face["persistedFaceId"] == faceId:
                        # Use encode() to convert the Unicode strings contained in JSON to ASCII.
                        name = face["userData"].encode('ascii', 'ignore')
                        break
            except Exception,e:
                pass
        return name

    def nameToFaceId(self, faceListId, name):
        faceId = ""
        decoded = self.getFaceList(faceListId)
        if decoded:
            try:
                for face in decoded["persistedFaces"]:
                    if face["userData"] == name:
                        # Use encode() to convert the Unicode strings contained in JSON to ASCII.
                        faceId = face["persistedFaceId"].encode('ascii', 'ignore')
                        break
            except Exception,e:
                pass
        return faceId


class MyClass(GeneratedClass):
    def __init__(self):
        global SpeechRecognizer, FaceDetector, ReactToTouch
        GeneratedClass.__init__(self)
        # Warning: module names must be global variables.
        # The name given to the constructor must be the name of the variable.

        # Enable one of the two next lines for Nao speech recognition or Google speech recognition
        #SpeechRecognizer = speechRecognitionNaoModule("SpeechRecognizer")
        SpeechRecognizer = speechRecognitionGoogleModule()

        FaceDetector = FaceDetectionModule("FaceDetector")
        ReactToTouch = ReactToTouchModule("ReactToTouch")
        self.faceRecognition = FaceRecognition()
        self.faceListId = "42"
        self.tts = ALProxy("ALTextToSpeech")
        self.doContinue = False

    def onLoad(self):
        #put initialization code here
        pass

    def onUnload(self):
        # Put clean-up code here.
        # This method will be called when the onInput_onStart() thread ends or when the behavior is stopped.
        # The behavior can be stopped in Choregraphe using the red 'Stop' button, or by using the 'Run Behavior'
        # box and activating the 'onStop' input. In Python this means the ALBehaviorManager method stopBehavior() is called.
        # First set self.doContinue to False otherwise the onInput_onStart() thread will continue if it is still running.
        self.doContinue = False
        # Important to call exit on the modules created, otherwise the next time ALModule.__init__(self, name)
        # will fail because the modules are already registered!
        # Use try...except here in case the (one of the) modules have already exitted.
        try:
            FaceDetector.stopFaceDetection()
            FaceDetector.stopFaceTracking()
            FaceDetector.stopSoundTracking()
            FaceDetector.exit()
        except Exception, e:
            pass

        # For Google speech recognition instead of Nao speech recognition disable the next try...except block.
#        try:
#            SpeechRecognizer.stopListening()
#            SpeechRecognizer.exit()
#        except Exception, e:
#            pass

        try:
            ReactToTouch.exit()
        except Exception, e:
            pass

    def onInput_onStart(self):
        global SpeechRecognizer, FaceDetector, ReactToTouch

        self.audiodevice = ALProxy("ALAudioDevice")
        self.audiodevice.setOutputVolume(50)

        while testConnected() == False:
            self.tts.say("not connected to the internet")
            if re.search('.*bumper.*', ReactToTouch.getTouch(), re.IGNORECASE):
                self.finish()
                return
            time.sleep(5)

        self.doContinue = True
        self.tts.say("let's greet some people")
        while self.doContinue:
            FaceDetector.startFaceDetection()
            FaceDetector.startSoundTracking()
            face = ""
            count = 0
            while self.doContinue:
                if count == 10:
                    FaceDetector.lookForward()
                    count = 0
                count = count + 1
                # Possibility to interrupt by touch.
                if re.search('.*bumper.*', ReactToTouch.getTouch(), re.IGNORECASE):
                    self.finish()
                    return
                print "getting face"
                face = FaceDetector.getFace();
                if face != "":
                    break
                time.sleep(0.5)
            # When the behaviour was stopped in Choregraphe by using the red 'Stop' button, onUnload() will be
            # called which will set self.doContinue to False. If this is the case, this thread must also be stopped.
            if self.doContinue == False:
                return
            print "face detected, going to take a picture"
            self.tts.say("let me check on you")
            FaceDetector.startFaceTracking()
            try:
                photoCapture = ALProxy("ALPhotoCapture")
                photoCapture.setResolution(3) # 1280x960 resolution
                photoCapture.setCameraID(0) # Top camera
                photoCapture.setPictureFormat("jpg")
                photoCapture.takePicture("/home/nao/reneb", "snapshot.jpg")
            except Exception, e:
                print "photoCapture exeption: " + str(e)
            newFaceId = self.faceRecognition.getFaceIdFromNewFace("/home/nao/reneb/snapshot.jpg")
            if newFaceId is not None:
                print "new face ID: " + newFaceId
                persistedFaceId, confidence = self.faceRecognition.getRecognizedFaceId(self.faceListId, newFaceId)
                print "recognized face ID: " + persistedFaceId + ", confidence: " + str(confidence)
                name = self.faceRecognition.faceIdToName(self.faceListId, persistedFaceId)
                print "\n***** recognized face name: " + name + ", confidence: " + str(confidence) + " *****\n"
                if self.doContinue and name == "":
                    self.tts.say("what is your name?")
                    name = SpeechRecognizer.speechToText()
                    # Possibility to interrupt by touch.
                    if re.search('.*bumper.*', ReactToTouch.getTouch(), re.IGNORECASE):
                        self.finish()
                        return

                    if self.doContinue and name != "":
                        self.tts.say("nice to meet you, " + name + ", please keep still for a moment")
                        if self.faceRecognition.addFaceToList(self.faceListId, "/home/nao/reneb/snapshot.jpg", name) == True:
                            self.tts.say("thank you, I will remember you!")
                        else:
                            self.tts.say("could not add face to facelist")
                elif self.doContinue:
                    # Uncomment the line below when running in Choregraphe.
                    self.onRecognized()
                    self.tts.say("hi again," + name)
            else:
                self.tts.say("sorry, could not take a good picture")
            time.sleep(10.0)

    def finish(self):
        self.doContinue = False
        # Uncomment the line below when running in Choregraphe.
        self.onStopped() #activate the output of the box

    def onInput_onStop(self):
        self.onUnload() #it is recommended to reuse the clean-up as the box is stopped
        self.onStopped() #activate the output of the box]]>
</content>
                            </script>
                            <Input name="onLoad" type="1" type_size="1" nature="0" inner="1" tooltip="Signal sent when diagram is loaded." id="1" />
                            <Input name="onStart" type="1" type_size="1" nature="2" inner="0" tooltip="Box behavior starts when a signal is received on this input." id="2" />
                            <Input name="onStop" type="1" type_size="1" nature="3" inner="0" tooltip="Box behavior stops when a signal is received on this input." id="3" />
                            <Output name="onStopped" type="1" type_size="1" nature="1" inner="0" tooltip="Signal sent when box behavior is finished." id="4" />
                            <Output name="onRecognized" type="1" type_size="1" nature="2" inner="0" tooltip="" id="5" />
                        </Box>
                        <Box name="Hello" id="2" localization="8" tooltip="Simple hello animation.&#x0A;&#x0A;!!Warning!! There is no speech in this box. It is a just an animation box with some&#x0A;leds animation." x="508" y="243">
                            <bitmap>media/images/box/movement/move.png</bitmap>
                            <script language="4">
                                <content>
                                    <![CDATA[class MyClass(GeneratedClass):
    def __init__(self):
        GeneratedClass.__init__(self, False)

    def onLoad(self):
        #~ puts code for box initialization here
        pass

    def onUnload(self):
        #~ puts code for box cleanup here
        pass

    def onInput_onStart(self):
        #~ self.onStopped() #~ activate output of the box
        pass

    def onInput_onStop(self):
        self.onUnload() #~ it is recommanded to call onUnload of this box in a onStop method, as the code written in onUnload is used to stop the box as well
        pass]]>
</content>
                            </script>
                            <Input name="onLoad" type="1" type_size="1" nature="0" inner="1" tooltip="Signal sent when Diagram is loaded." id="1" />
                            <Input name="onStart" type="1" type_size="1" nature="2" inner="0" tooltip="Box behavior starts when a signal is received on this input." id="2" />
                            <Input name="onStop" type="1" type_size="1" nature="3" inner="0" tooltip="Box behavior stops when a signal is received on this input." id="3" />
                            <Output name="onStopped" type="1" type_size="1" nature="1" inner="0" tooltip="Signal sent when box behavior is finished." id="4" />
                            <Timeline enable="1" fps="25" start_frame="1" end_frame="-1" size="115">
                                <BehaviorLayer name="behavior_layer1">
                                    <BehaviorKeyframe name="FaceLeds" index="1">
                                        <Diagram>
                                            <Box name="Light_AskForAttentionEyes" id="1" localization="8" tooltip="Set an animated gaze which calls for attention (purple eyes).&#x0A;&#x0A;Note: It is a never ending box. You must call onStop to stop it.&#x0A;&#x0A;*** state: 5a_release ***&#x0A;*** ref box in: 5a_release\Leds\Light_AskForAttentionEyes\Light_AskForAttentionEyes.xar ***&#x0A;*** last modification date(svn): Version it! ***" x="281" y="144">
                                                <bitmap>media/images/box/interaction/LED.png</bitmap>
                                                <script language="4">
                                                    <content>
                                                        <![CDATA[class MyClass(GeneratedClass):
  def __init__(self):
    GeneratedClass.__init__(self, False)

  def onLoad(self):
    self.bIsRunning = False;
    self.leds = ALProxy("ALLeds")

  def onUnload(self):
    self.onInput_onStop(); # will stop current loop execution

  def onInput_onStart(self):
    #self.logger.info( self.getName() + ": start - begin" );

    if( self.bIsRunning ):
      #print( self.getName() + ": already started => nothing" );
      return;

    self.bIsRunning = True;
    self.bMustStop = False;

    rDuration = 0.2;
    self.leds.post.fadeRGB( "FaceLedsTop", 0xff00ff, rDuration );
    self.leds.post.fadeRGB( "FaceLedsInternal", 0xff00ff, rDuration );
    self.leds.post.fadeRGB( "FaceLedsBottom", 0xff00ff, rDuration );
    self.leds.fadeRGB( "FaceLedsExternal", 0xff00ff, rDuration );

    while( not self.bMustStop ):
      rTime = 0.1;
      self.leds.post.fadeRGB( "FaceLedsTop", 0xffffff, rTime );
      self.leds.fadeRGB( "FaceLedsBottom", 0xffffff, rTime );
      if( self.bMustStop ):
        break;
      rTime = 0.3
      self.leds.post.fadeRGB( "FaceLedsTop", 0xff00ff, rTime );
      self.leds.fadeRGB( "FaceLedsBottom", 0xff00ff, rTime );


    # end while
    self.bIsRunning = False;
    self.onStopped();

  def onInput_onStop(self):
    self.bMustStop = True; # will stop current loop execution]]>
</content>
                                                </script>
                                                <Input name="onLoad" type="1" type_size="1" nature="0" inner="1" tooltip="Signal sent when diagram is loaded." id="1" />
                                                <Input name="onStart" type="1" type_size="1" nature="2" inner="0" tooltip="Box behavior starts when a signal is received on this input." id="2" />
                                                <Input name="onStop" type="1" type_size="1" nature="3" inner="0" tooltip="Box behavior stops when a signal is received on this input." id="3" />
                                                <Output name="onStopped" type="1" type_size="1" nature="1" inner="0" tooltip="Signal sent when box behavior is finished." id="4" />
                                            </Box>
                                            <Link inputowner="1" indexofinput="2" outputowner="0" indexofoutput="1" />
                                        </Diagram>
                                    </BehaviorKeyframe>
                                </BehaviorLayer>
                                <ActuatorList model="Nao">
                                    <ActuatorCurve name="value" actuator="HeadYaw" recordable="1" mute="0" unit="-1">
                                        <Key frame="20" value="-7.73688" />
                                        <Key frame="39" value="-20.1296" />
                                        <Key frame="56" value="-23.8211" />
                                        <Key frame="70" value="-23.9968" />
                                        <Key frame="87" value="-29.7977" />
                                        <Key frame="115" value="-21.5359" />
                                    </ActuatorCurve>
                                    <ActuatorCurve name="value" actuator="HeadPitch" recordable="1" mute="0" unit="-1">
                                        <Key frame="20" value="16.9607" />
                                        <Key frame="39" value="-9.75839" />
                                        <Key frame="56" value="-19.5144" />
                                        <Key frame="70" value="-3.43018" />
                                        <Key frame="87" value="-11.0768" />
                                        <Key frame="115" value="-0.617646" />
                                    </ActuatorCurve>
                                    <ActuatorCurve name="value" actuator="LShoulderPitch" recordable="1" mute="0" unit="-1">
                                        <Key frame="18" value="64.0707" />
                                        <Key frame="37" value="53.1721" />
                                        <Key frame="54" value="53.8752" />
                                        <Key frame="68" value="49.3927" />
                                        <Key frame="85" value="51.4143" />
                                        <Key frame="113" value="48.2502" />
                                    </ActuatorCurve>
                                    <ActuatorCurve name="value" actuator="LShoulderRoll" recordable="1" mute="0" unit="-1">
                                        <Key frame="18" value="20.8279" />
                                        <Key frame="37" value="13.0056" />
                                        <Key frame="54" value="11.6872" />
                                        <Key frame="68" value="12.4782" />
                                        <Key frame="85" value="14.2361" />
                                        <Key frame="113" value="13.0056" />
                                    </ActuatorCurve>
                                    <ActuatorCurve name="value" actuator="LElbowYaw" recordable="1" mute="0" unit="-1">
                                        <Key frame="18" value="-46.0577" />
                                        <Key frame="37" value="-39.6416" />
                                        <Key frame="54" value="-38.9384" />
                                        <Key frame="68" value="-34.9833" />
                                        <Key frame="85" value="-43.1572" />
                                        <Key frame="113" value="-38.4111" />
                                    </ActuatorCurve>
                                    <ActuatorCurve name="value" actuator="LElbowRoll" recordable="1" mute="0" unit="-1">
                                        <Key frame="18" value="-79.0123" />
                                        <Key frame="37" value="-73.9145" />
                                        <Key frame="54" value="-67.7621" />
                                        <Key frame="68" value="-71.5415" />
                                        <Key frame="85" value="-75.5845" />
                                        <Key frame="113" value="-67.85" />
                                    </ActuatorCurve>
                                    <ActuatorCurve name="value" actuator="LWristYaw" recordable="1" mute="0" unit="-1">
                                        <Key frame="37" value="8.4352" />
                                        <Key frame="113" value="6.85315" />
                                    </ActuatorCurve>
                                    <ActuatorCurve name="value" actuator="LHand" recordable="1" mute="0" unit="-1">
                                        <Key frame="37" value="0.238207" />
                                        <Key frame="113" value="0.240025" />
                                    </ActuatorCurve>
                                    <ActuatorCurve name="value" actuator="RShoulderPitch" recordable="1" mute="0" unit="-1">
                                        <Key frame="16" value="14.153" />
                                        <Key frame="35" value="-67.1469" />
                                        <Key frame="52" value="-62.4007" />
                                        <Key frame="66" value="-72.2446" />
                                        <Key frame="83" value="-65.8285" />
                                        <Key frame="111" value="58.4504" />
                                    </ActuatorCurve>
                                    <ActuatorCurve name="value" actuator="RShoulderRoll" recordable="1" mute="0" unit="-1">
                                        <Key frame="16" value="-13.8893" />
                                        <Key frame="35" value="-54.6711" />
                                        <Key frame="52" value="-26.3699" />
                                        <Key frame="66" value="-55.0226" />
                                        <Key frame="83" value="-18.8112" />
                                        <Key frame="111" value="-14.3288" />
                                    </ActuatorCurve>
                                    <ActuatorCurve name="value" actuator="RElbowYaw" recordable="1" mute="0" unit="-1">
                                        <Key frame="16" value="-17.9323" />
                                        <Key frame="35" value="32.3418" />
                                        <Key frame="52" value="22.41" />
                                        <Key frame="66" value="19.949" />
                                        <Key frame="83" value="21.8826" />
                                        <Key frame="93" value="56" />
                                        <Key frame="111" value="47.3712" />
                                    </ActuatorCurve>
                                    <ActuatorCurve name="value" actuator="RElbowRoll" recordable="1" mute="0" unit="-1">
                                        <Key frame="16" value="79.3686" />
                                        <Key frame="35" value="13.8893" />
                                        <Key frame="42" value="20" />
                                        <Key frame="52" value="53.5285" />
                                        <Key frame="60" value="39" />
                                        <Key frame="66" value="11" />
                                        <Key frame="76" value="15" />
                                        <Key frame="83" value="40.5205" />
                                        <Key frame="93" value="58.4" />
                                        <Key frame="111" value="72.5131" />
                                    </ActuatorCurve>
                                    <ActuatorCurve name="value" actuator="RWristYaw" recordable="1" mute="0" unit="-1">
                                        <Key frame="35" value="-17.9323" />
                                        <Key frame="83" value="-17.405" />
                                        <Key frame="111" value="10.4567" />
                                    </ActuatorCurve>
                                    <ActuatorCurve name="value" actuator="RHand" recordable="1" mute="0" unit="-1">
                                        <Key frame="35" value="0.853478" />
                                        <Key frame="83" value="0.854933" />
                                        <Key frame="111" value="0.425116" />
                                    </ActuatorCurve>
                                </ActuatorList>
                            </Timeline>
                            <Resource name="Head" type="Lock" timeout="0" />
                            <Resource name="Arms" type="Lock" timeout="0" />
                            <Resource name="Left eye leds" type="Lock" timeout="0" />
                            <Resource name="Right eye leds" type="Lock" timeout="0" />
                        </Box>
                        <Box name="Set Stiffness" id="7" localization="8" tooltip="Stiffen the motors selected in parameters." x="244" y="564">
                            <bitmap>media/images/box/movement/stiffness.png</bitmap>
                            <script language="4">
                                <content>
                                    <![CDATA[import time

class MyClass(GeneratedClass):
    def __init__(self):
        GeneratedClass.__init__(self, False)

    def onLoad(self):
        self.bIsRunning = False
        self.motion = ALProxy("ALMotion")

    def onUnload(self):
        self.bIsRunning = False

    def setStiffness(self, stiffness):
        self.bIsRunning = True
        duration = self.getParameter("Duration (s)")
        if(self.getParameter("Head")):
            self.motion.post.stiffnessInterpolation("Head", stiffness, duration)
        if(self.getParameter("Left arm")):
            self.motion.post.stiffnessInterpolation("LArm", stiffness, duration)
        if(self.getParameter("Right arm")):
            self.motion.post.stiffnessInterpolation("RArm", stiffness, duration)
        if(self.getParameter("Left leg")):
            self.motion.post.stiffnessInterpolation("LLeg", stiffness, duration)
        if(self.getParameter("Right leg")):
            self.motion.post.stiffnessInterpolation("RLeg", stiffness, duration)
        time.sleep(duration)
        self.bIsRunning = False

    def onInput_onSet(self):
        if( self.bIsRunning ):
            return
        self.setStiffness(self.getParameter("Motors stiffness (%)")/100.)
        self.onReady() #~ activate output of the box]]>
</content>
                            </script>
                            <Input name="onLoad" type="1" type_size="1" nature="0" inner="1" tooltip="Signal sent when diagram is loaded." id="1" />
                            <Input name="onSet" type="1" type_size="1" nature="1" inner="0" tooltip="When this input is stimulated, the stiffness of the selected motors is set to&#x0A;the stiffness parameter value." id="2" />
                            <Output name="onReady" type="1" type_size="1" nature="2" inner="0" tooltip="Signal sent when stiffness has been set." id="3" />
                            <Parameter name="Head" inherits_from_parent="0" content_type="0" value="0" default_value="1" tooltip="Do we want to modify the head stiffness?" id="4" />
                            <Parameter name="Left arm" inherits_from_parent="0" content_type="0" value="0" default_value="1" tooltip="Do we want to modify the left arm stiffness?" id="5" />
                            <Parameter name="Right arm" inherits_from_parent="0" content_type="0" value="1" default_value="1" tooltip="Do we want to modify the right arm stiffness?" id="6" />
                            <Parameter name="Left leg" inherits_from_parent="0" content_type="0" value="0" default_value="1" tooltip="Do we want to modify the left leg stiffness?" id="7" />
                            <Parameter name="Right leg" inherits_from_parent="0" content_type="0" value="0" default_value="1" tooltip="Do we want to modify the right leg stiffness?" id="8" />
                            <Parameter name="Motors stiffness (%)" inherits_from_parent="0" content_type="1" value="100" default_value="100" min="0" max="100" tooltip="Stiffness value the motors are set to." id="9" />
                            <Parameter name="Duration (s)" inherits_from_parent="0" content_type="2" value="1" default_value="1" min="0.02" max="10" tooltip="How much time to go to the max/min stiffness in seconds." id="10" />
                        </Box>
                        <Box name="SetVolume" id="3" localization="8" tooltip="" x="337" y="381">
                            <bitmap>media/images/box/box-python-script.png</bitmap>
                            <script language="4">
                                <content>
                                    <![CDATA[# -*- encoding: UTF-8 -*-
""" Say 'hello, you' each time a human face is detected

"""

from naoqi import ALProxy
from naoqi import ALBroker
from naoqi import ALModule


# Global variables. Module names must be global otherwise the callback functions will report for example:
# 'python object not found FaceDetector'.
SetVolume = None


class SetVolumeModule(ALModule):
    """ A simple module able to react to sensor events.
    Leave this doc string else this module will not be bound!

    """
    def __init__(self, name):
        ALModule.__init__(self, name)
        # No need for IP and port here because
        # we have our Python broker connected to NAOqi broker
        self.memory = ALProxy("ALMemory")
        # Unsubscribe to event if still subscribed.
        try:
            self.memory.unsubscribeToEvent("FrontTactilTouched", "SetVolume")
            self.memory.unsubscribeToEvent("RearTactilTouched", "SetVolume")
        except Exception, e:
            pass
        self.memory.subscribeToEvent("FrontTactilTouched", "SetVolume", "onFrontTouched")
        self.memory.subscribeToEvent("RearTactilTouched", "SetVolume", "onRearTouched")
        self.audiodevice = ALProxy("ALAudioDevice")
        self.tts = ALProxy("ALTextToSpeech")

    def onFrontTouched(self, strVarName, value):
        # Only react on touch down event.
        if value == 1.0:
            self.audiodevice.setOutputVolume(min(100, self.audiodevice.getOutputVolume() + 10.0))
            self.tts.say("volume up")

    def onRearTouched(self, strVarName, value):
        # Only react on touch down event.
        if value == 1.0:
            self.audiodevice.setOutputVolume(max(0, self.audiodevice.getOutputVolume() - 10.0))
            self.tts.say("volume down")


class MyClass(GeneratedClass):
    def __init__(self):
        global SetVolume
        GeneratedClass.__init__(self)
        # Warning: module names must be global variables.
        # The name given to the constructor must be the name of the variable.
        SetVolume = SetVolumeModule("SetVolume")

    def onLoad(self):
        #put initialization code here
        pass

    def onUnload(self):
        # Put clean-up code here.
        # This method will be called when the onInput_onStart() thread ends or when the behavior is stopped.
        # The behavior can be stopped in Choregraphe using the red 'Stop' button, or by using the 'Run Behavior'
        # box and activating the 'onStop' input. In Python this means the ALBehaviorManager method stopBehavior() is called.
        # Important to call exit on the modules created, otherwise the next time ALModule.__init__(self, name)
        # will fail because the modules are already registered!
        # Use try...except here in case the (one of the) modules have already exitted.
        try:
            SetVolume.exit()
        except Exception, e:
            pass

    def onInput_onStart(self):
        pass

    def onInput_onStop(self):
        self.onUnload() #it is recommended to reuse the clean-up as the box is stopped
        self.onStopped() #activate the output of the box]]>
</content>
                            </script>
                            <Input name="onLoad" type="1" type_size="1" nature="0" inner="1" tooltip="Signal sent when diagram is loaded." id="1" />
                            <Input name="onStart" type="1" type_size="1" nature="2" inner="0" tooltip="Box behavior starts when a signal is received on this input." id="2" />
                            <Input name="onStop" type="1" type_size="1" nature="3" inner="0" tooltip="Box behavior stops when a signal is received on this input." id="3" />
                            <Output name="onStopped" type="1" type_size="1" nature="1" inner="0" tooltip="Signal sent when box behavior is finished." id="4" />
                        </Box>
                        <Link inputowner="2" indexofinput="2" outputowner="1" indexofoutput="5" />
                        <Link inputowner="1" indexofinput="2" outputowner="0" indexofoutput="2" />
                        <Link inputowner="0" indexofinput="4" outputowner="1" indexofoutput="4" />
                        <Link inputowner="7" indexofinput="2" outputowner="0" indexofoutput="2" />
                        <Link inputowner="3" indexofinput="2" outputowner="0" indexofoutput="2" />
                    </Diagram>
                </BehaviorKeyframe>
            </BehaviorLayer>
        </Timeline>
    </Box>
</ChoregrapheProject>
